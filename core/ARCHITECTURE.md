# ğŸ—ï¸ SymBro â€“ ArchitekturÃ¼bersicht

## ğŸ“‚ Projektstruktur

```
SymBro/
â”œâ”€â”€ core/
â”‚   â””â”€â”€ ARCHITECTURE.md           # â†’ ArchitekturÃ¼bersicht (dieses Dokument)
â”œâ”€â”€ data/
â”‚   â””â”€â”€ rlhf/
â”‚       â””â”€â”€ logs/
â”‚           â””â”€â”€ interactions.json # â†’ User-Interaktionen (Feedback + Rewards)
â”œâ”€â”€ gui_pyside/                   # â†’ GUI auf Basis von PySide6
â”œâ”€â”€ modules/
â”‚   â””â”€â”€ rlhf_engine.py            # â†’ RLHF-Logik (Q-Learning, Replay Buffer, PrioritÃ¤ten)
â”‚   â””â”€â”€ replay_buffer.py          # â†’ Prioritized Experience Replay Buffer
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ embedding_analyzer.py     # â†’ Embedding-Analyse & Outlier Detection
â”‚   â””â”€â”€ intent_selector.py        # â†’ Intent-Erkennung & semantisches Routing
â”‚   â””â”€â”€ duplicate_remover.py      # â†’ Duplikat-Entferner fÃ¼r interactions.json
â”œâ”€â”€ tests/                        # â†’ Unittests (aktuell in Planung)
â”œâ”€â”€ README.md                     # â†’ Projektbeschreibung & Setup
```

---

## ğŸ§  Kernkomponenten

### 1. RLHF-Engine (Reinforcement Learning from Human Feedback)
- **Replay Buffer:** Prioritized Experience Replay (PER) mit TD-Error als PrioritÃ¤t.
- **TD-Berechnung:** Simple Q-Learning-Ansatz (Plan: Ausbau zu DDQN + LSTM).
- **Feedback-Integration:** Positive/negative User-Bewertungen steuern die Reward-Logik.
- **Belohnungsberechnung:**  
  - Manuelles Feedback: `Daumen hoch/runter`
  - Geplante Erweiterung: Automatisierte Rewards via semantisches Matching, Confidence, Embedding-Distanz.

---

## ğŸ’¬ Dialogsystem & GPT-Integration

- GPT-3.5-Turbo Integration fÃ¼r dynamische Antworten.
- Kontextlogik: Ãœbergabe der letzten 10 relevanten Interaktionen.
- System-Prompt unterstÃ¼tzt:
  - Situativen Stil
  - HÃ¶flichen Widerspruch
  - Anpassung an Emoji-Stimmung und User-PrÃ¤ferenzen (z.â€¯B. Lieblingsfarbe)

---

## ğŸ§© ZusÃ¤tzliche Module

| Modul                    | Funktion                                |
|--------------------------|-----------------------------------------|
| `embedding_analyzer.py`   | Outlier Detection fÃ¼r Interaktionsdaten (VAE geplant) |
| `intent_selector.py`      | Skill-Routing auf Basis semantischer Analyse |
| `duplicate_remover.py`    | Entfernt doppelte EintrÃ¤ge aus `interactions.json`   |

---

## ğŸš€ Geplante Weiterentwicklung

- Deep Q-Network (DDQN) + LSTM Agent.
- Prioritized Experience Replay mit TD-Error und Zeitgewichtung.
- Outlier Filtering via Variational Autoencoder (VAE).
- Random Network Distillation (RND) zur Unsicherheitsmessung.
- Reward-Optimierung durch semantisches Matching & Confidence Scores.
- GUI-Feintuning (runde Sprechblasen, kleinere Icons).
- Reinforcement Learning from Human Feedback (RLHF) mit wachsendem Mitspracherecht des Bots.
- PersÃ¶nliches Profiling via `user_insight.json`.

---

## ğŸ§© Zielbild

SymBro soll langfristig ein smarter, lernfÃ¤higer Desktop-Begleiter werden, der:
- Entscheidungen situativ und kontextabhÃ¤ngig trifft.
- Feedback aktiv in sein Lernverhalten einbezieht.
- Eine echte Mensch-KI-Symbiose anstrebt.
